
model_name: "meta-llama/Llama-2-7b-hf"
training_config:
  r: 8
  lora_alpha: 32
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  lora_dropout: 0.1
  bias: "none"
  task_type: "SEQUENCE_CLASSIFICATION"
training:
  batch_size: 8
  gradient_accumulation_steps: 1
  learning_rate: 3e-4
  betas: [0.9, 0.98]
  weight_decay: 1e-3
  eps: 1e-8
  epochs: 2
